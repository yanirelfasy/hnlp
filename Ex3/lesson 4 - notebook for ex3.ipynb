{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook enables comparing stanza and YAP for Hebrew text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 21.3MB/s]\n",
      "2021-05-15 14:08:27 INFO: Downloading default packages for language: he (Hebrew)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/he/default.zip: 100%|█████| 208M/208M [01:24<00:00, 2.45MB/s]\n",
      "2021-05-15 14:09:58 INFO: Finished downloading models and saved to C:\\Users\\yelfs\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import stanza\n",
    "stanza.download('he')\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "from pandas.io.json import json_normalize\n",
    "sys.path.append(\"C:\\HNLP\\Ex3\\YAP-Wrapper-master\")\n",
    "from yap_api import YapApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    def __init__(self):\n",
    "        self.heb_nlp = stanza.Pipeline(lang='he', processors='tokenize,mwt,pos,lemma,depparse')\n",
    "        #replace MY_TOKEN with the token you got from the langndata website\n",
    "        self.yap_token=\"a073769bd24ead12455d64d220c0287b\"\n",
    "        self.ip='127.0.0.1:8000'\n",
    "        self.yap=YapApi() \n",
    "    \n",
    "    def print_stanza_analysis(self, text, name):\n",
    "        text += \" XX\"\n",
    "        doc=self.heb_nlp(text)\n",
    "        lst=[]\n",
    "        for sen in doc.sentences:\n",
    "            for token in sen.tokens:\n",
    "                for word in token.words:\n",
    "                    features=[(word.text,\n",
    "                               word.lemma,\n",
    "                               word.upos,\n",
    "                               word.xpos,\n",
    "                               word.head,\n",
    "                               word.deprel,\n",
    "                               word.feats)]\n",
    "\n",
    "                    df=pd.DataFrame(features, columns=[\"text\", \"lemma\", \"upos\", \"xpos\", \"head\", \"deprel\",\"feats\"])\n",
    "                    lst.append(df)\n",
    "        tot_df=pd.concat(lst, ignore_index=True)\n",
    "        tot_df=tot_df.shift(1).iloc[1:]\n",
    "        tot_df[\"head\"]=tot_df[\"head\"].astype(int)\n",
    "        tot_df.head(50).to_csv(name, encoding=\"utf-8\")\n",
    "        print(tot_df.head(50))\n",
    "        \n",
    "    def print_yap_analysis(self, text, name):\n",
    "        text= text.replace(r'\"', r'\\\"')\n",
    "        tokenized_text, segmented_text, lemmas, dep_tree, md_lattice, ma_lattice=self.yap.run(text, self.ip)                 \n",
    "        dep_tree.set_index(\"num\",inplace=True)\n",
    "        dep_tree.to_csv(name, encoding=\"utf-8\")\n",
    "        print(dep_tree)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-15 17:55:05 INFO: Loading these models for language: he (Hebrew):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | htb     |\n",
      "| mwt       | htb     |\n",
      "| pos       | htb     |\n",
      "| lemma     | htb     |\n",
      "| depparse  | htb     |\n",
      "=======================\n",
      "\n",
      "2021-05-15 17:55:05 INFO: Use device: cpu\n",
      "2021-05-15 17:55:05 INFO: Loading: tokenize\n",
      "2021-05-15 17:55:05 INFO: Loading: mwt\n",
      "2021-05-15 17:55:05 INFO: Loading: pos\n",
      "2021-05-15 17:55:05 INFO: Loading: lemma\n",
      "2021-05-15 17:55:05 INFO: Loading: depparse\n",
      "2021-05-15 17:55:06 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza\n",
      "    text lemma   upos   xpos  head     deprel  \\\n",
      "1      ה     ה    DET    DET     2    det:def   \n",
      "2   משפט  משפט   NOUN   NOUN     0       root   \n",
      "3      ש     ש  SCONJ  SCONJ     6       mark   \n",
      "4  אנחנו   הוא   PRON   PRON     6      nsubj   \n",
      "5      ה     ה    DET    DET     6       mark   \n",
      "6  מצאנו   מצא   VERB   VERB     2  acl:relcl   \n",
      "7      .     .  PUNCT  PUNCT     2      punct   \n",
      "\n",
      "                                               feats  \n",
      "1                                       PronType=Art  \n",
      "2                            Gender=Masc|Number=Sing  \n",
      "3                                               None  \n",
      "4  Gender=Fem,Masc|Number=Plur|Person=1|PronType=Prs  \n",
      "5                                       PronType=Art  \n",
      "6  Gender=Fem,Masc|HebBinyan=PAAL|Number=Plur|Per...  \n",
      "7                                               None  \n",
      "------------------------\n",
      "yap\n",
      "Start Yap call\n",
      "Tokens: 4\n",
      "End Yap call 0 /0\n",
      "      word lemma    pos  pos_2  \\\n",
      "num                              \n",
      "1        ה     ה    DEF    DEF   \n",
      "2     משפט  משפט     NN     NN   \n",
      "3        ש     ש    REL    REL   \n",
      "4    אנחנו   הוא    PRP    PRP   \n",
      "5        ה     ה    DEF    DEF   \n",
      "6    מצאנו  מצאי     NN     NN   \n",
      "7        .     .  yyDOT  yyDOT   \n",
      "\n",
      "                                                 empty dependency_arc  \\\n",
      "num                                                                     \n",
      "1                                                                   2   \n",
      "2                                          gen=M|num=S              0   \n",
      "3                                                                   2   \n",
      "4                              gen=F|gen=M|num=P|per=1              6   \n",
      "5                                                                   6   \n",
      "6    gen=M|num=S|suf_gen=F|suf_gen=M|suf_num=P|suf_...              3   \n",
      "7                                                                   2   \n",
      "\n",
      "    dependency_part dependency_arc_2 dependency_part_2  \n",
      "num                                                     \n",
      "1               def                _                 _  \n",
      "2              ROOT                _                 _  \n",
      "3             rcmod                _                 _  \n",
      "4              subj                _                 _  \n",
      "5               def                _                 _  \n",
      "6           relcomp                _                 _  \n",
      "7             punct                _                 _  \n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"\"\"\n",
    "    המשפט שאנחנו המצאנו.\n",
    "    \"\"\"\n",
    "    ]\n",
    "for index, text in enumerate(texts):\n",
    "    processor=Processor()\n",
    "    print(\"stanza\")\n",
    "    name = str(index) + '_STA.csv'\n",
    "    processor.print_stanza_analysis(text, name)\n",
    "    print(\"------------------------\")\n",
    "    print(\"yap\")\n",
    "    name = str(index) + '_YAP.csv'\n",
    "    processor.print_yap_analysis(text, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hebnlp",
   "language": "python",
   "name": "hebnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
